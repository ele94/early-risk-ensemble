{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisitos: Preprocessing, featurize\n",
    "\n",
    "En esta libreta cargamos los features textuales, calculamos los embeddings, y entrenamos el modelo de deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Librer√≠as, funciones, random seed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow.keras.models\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tokenizer import tokenizer as reddit_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utiles\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "pickle_path = \"/datos/ecampillo/jupyter/dl-notebooks/pickles\"\n",
    "\n",
    "def logger(message, debug_file=\"log.txt\"):\n",
    "    print(message)\n",
    "    original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "    with open(debug_file, 'a') as f:\n",
    "        sys.stdout = f # Change the standard output to the file we created.\n",
    "        print(message)\n",
    "        sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "        \n",
    "def save_pickle(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with open(file, 'wb') as data_file:\n",
    "        pickle.dump(data, data_file)\n",
    "        \n",
    "def load_pickle(filepath, filename):\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with open(file, 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 42\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow\n",
    "import sys\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "tensorflow.random.set_seed(42) \n",
    "logger(\"Initialized numpy random and tensorflow random seed at 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle(pickle_path, \"X_train.pkl\")\n",
    "X_test = load_pickle(pickle_path, \"X_test.pkl\")\n",
    "y_train = load_pickle(pickle_path, \"y_train.pkl\")\n",
    "y_test = load_pickle(pickle_path, \"y_test.pkl\")\n",
    "\n",
    "feats_train = load_pickle(pickle_path, \"feats_train.pkl\")\n",
    "feats_test = load_pickle(pickle_path, \"feats_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the Embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.32 s, sys: 78.6 ms, total: 6.4 s\n",
      "Wall time: 6.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000) # 5000\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 50000  # podria ser la que quisieramos  # antes tenia 10000, voy a probar con 50000\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 933 ms, total: 11.6 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading the GloVe embeddings\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('/datos/erisk/deep-learning/embeddings/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 86.8 ms, total: 249 ms\n",
      "Wall time: 247 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating an embedding matrix\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(pickle_path, \"embedding_matrix.pkl\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM model (takes too long to train)\n",
    "meta_input = Input(shape=(1,))\n",
    "nlp_input = Input(shape=(maxlen,)) \n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(128))(emb)\n",
    "concat = concatenate([nlp_out, meta_input])\n",
    "classifier = Dense(32, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(classifier)\n",
    "model = Model(inputs=[nlp_input , meta_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "meta_input = Input(shape=(1,))\n",
    "nlp_input = Input(shape=(maxlen,))\n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "nlp_out = Conv1D(64, 5, activation='relu')(emb)\n",
    "max_pool = GlobalMaxPooling1D()(nlp_out)\n",
    "concat = concatenate([max_pool, meta_input])\n",
    "classifier = Dense(32, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(classifier)\n",
    "model = Model(inputs=[nlp_input, meta_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "239/239 [==============================] - 53s 219ms/step - loss: 151.0011 - acc: 0.5900 - val_loss: 339.7573 - val_acc: 0.0083\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 52s 218ms/step - loss: 56.1027 - acc: 0.5628 - val_loss: 42.2259 - val_acc: 0.3750\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 52s 219ms/step - loss: 36.4496 - acc: 0.6485 - val_loss: 302.0232 - val_acc: 0.0917\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 52s 218ms/step - loss: 38.1090 - acc: 0.6883 - val_loss: 0.1535 - val_acc: 0.9583\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 53s 220ms/step - loss: 32.8660 - acc: 0.6925 - val_loss: 1.3178e-11 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 52s 219ms/step - loss: 52.1140 - acc: 0.7050 - val_loss: 65.8504 - val_acc: 0.5333\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 52s 220ms/step - loss: 32.8407 - acc: 0.7615 - val_loss: 29.7271 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 52s 218ms/step - loss: 18.6709 - acc: 0.8347 - val_loss: 145.6614 - val_acc: 0.5167\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 52s 217ms/step - loss: 13.9636 - acc: 0.8724 - val_loss: 4.4893e-10 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 52s 218ms/step - loss: 12.7609 - acc: 0.8745 - val_loss: 4.2306e-14 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, feats_train.values()], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
