{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a combinar una red neuronal sencilla (por ejemplo, CNN) con embeddings CON features de tipo numéricos.\n",
    "Pasos:\n",
    "1. Cargando los datos\n",
    "2. Calculando los features\n",
    "3. Generando los embeddings\n",
    "4. Creando y entrenando la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Librerías, random seed, inicializaciones varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes, ensemble\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizer import tokenizer as reddit_tokenizer\n",
    "#from redditscore.tokenizer import CrazyTokenizer #https://github.com/crazyfrogspb/RedditScore\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "# my modules\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing\n"
     ]
    }
   ],
   "source": [
    "logger(\"Starting preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando la Random Seed a 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 42\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "tensorflow.random.set_seed(42) \n",
    "logger(\"Initialized numpy random and tensorflow random seed at 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargando los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones de preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the text\n",
    "import redditcleaner\n",
    "\n",
    "R_tokenizer = reddit_tokenizer.TweetTokenizer(preserve_case=False, preserve_url=False,\n",
    "                                    regularize=True, preserve_emoji=False, preserve_hashes=False,\n",
    "                                   preserve_handles=False)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    tokens = R_tokenizer.tokenize(sen)\n",
    "    sentence = \" \".join(tokens)\n",
    "    return sentence\n",
    "\n",
    "def preprocess_text_v2(sen):\n",
    "    # Cleaning reddit text\n",
    "    sentence = clean_reddit_text(sen)\n",
    "    \n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sentence)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    # sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # Lower case\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def clean_reddit_text(text):\n",
    "    return redditcleaner.clean(text)\n",
    "\n",
    "# new\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text)\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "# text tiene que venir en tokens\n",
    "def pos_tag_text(text):\n",
    "    text = nltk.pos_tag(text)\n",
    "    return text\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def remove_stopwords(text):\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    return text\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "def stemmize_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    stems = [ps.stem(w) for w in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def load_golden_truth(g_path, test_collection=False):\n",
    "    g_truth = {line.split()[0]: int(line.split()[1]) for line in open(g_path)}\n",
    "    if test_collection:\n",
    "        new_g_truth = {}\n",
    "        for user, truth in g_truth.items():\n",
    "            new_g_truth[\"test\"+user] = truth\n",
    "    else:\n",
    "        new_g_truth = g_truth.copy()\n",
    "    return new_g_truth\n",
    "\n",
    "def load_user_data(path, g_truth, test_collection=False):\n",
    "    #users = {}\n",
    "    user_writings = []\n",
    "    for filename in os.listdir(path):\n",
    "        old_user, file_extension = os.path.splitext(filename)\n",
    "        \n",
    "        if test_collection:\n",
    "            user = \"test\"+str(old_user)\n",
    "        else:\n",
    "            user = str(old_user)\n",
    "        tree = ET.parse(os.path.join(path, filename))\n",
    "        root = tree.getroot()\n",
    "        #user_writings = []\n",
    "        \n",
    "        for writing in root.findall('WRITING'):\n",
    "            title, text, date = \"\", \"\", \"\"\n",
    "            if writing.find('TITLE') is not None:\n",
    "                title = writing.find('TITLE').text\n",
    "                if title is None:\n",
    "                    title = \"\"\n",
    "            if writing.find('TEXT') is not None:\n",
    "                text = writing.find('TEXT').text\n",
    "                if text is None:\n",
    "                    text = \"\"\n",
    "                    \n",
    "            if len(title) > 0:\n",
    "                user_writing = {\"text\": title + \". \" + text, \"user\": user, \"g_truth\": g_truth[user]}\n",
    "            else:\n",
    "                user_writing = {\"text\": text, \"user\": user, \"g_truth\": g_truth[user]}\n",
    "            user_writings.append(user_writing)\n",
    "        #users[user] = user_writings\n",
    "        \n",
    "    return user_writings\n",
    "\n",
    "def load_joined_user_data(path, g_truth, test_collection=False):\n",
    "    user_writings = []\n",
    "    for filename in os.listdir(path):\n",
    "        old_user, file_extension = os.path.splitext(filename)\n",
    "        \n",
    "        if test_collection:\n",
    "            user = \"test\"+str(old_user)\n",
    "        else:\n",
    "            user = str(old_user)\n",
    "        tree = ET.parse(os.path.join(path, filename))\n",
    "        root = tree.getroot()\n",
    "        writings = []\n",
    "        \n",
    "        for writing in root.findall('WRITING'):\n",
    "            title, text, date = \"\", \"\", \"\"\n",
    "            if writing.find('TITLE') is not None:\n",
    "                title = writing.find('TITLE').text\n",
    "                if title is None:\n",
    "                    title = \"\"\n",
    "            if writing.find('TEXT') is not None:\n",
    "                text = writing.find('TEXT').text\n",
    "                if text is None:\n",
    "                    text = \"\"\n",
    "                    \n",
    "            if len(title) > 0:\n",
    "                writings.append(title + \". \" + text)\n",
    "            else:\n",
    "                writings.append(text)\n",
    "        writings = \". \".join(writings)\n",
    "        user_writings.append({\"text\": writings, \"user\": user, \"g_truth\": g_truth[user]})\n",
    "        #users[user] = user_writings\n",
    "        \n",
    "    return user_writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_file = \"/datos/erisk/deep-learning/data/erisk2021_training_data/data\"\n",
    "test_users_file = \"/datos/erisk/deep-learning/data/erisk2021_test_data/data\"\n",
    "\n",
    "train_g_truth_file = \"/datos/erisk/deep-learning/data/erisk2021_training_data/golden_truth.txt\"\n",
    "test_g_truth_file = \"/datos/erisk/deep-learning/data/erisk2021_test_data/golden_truth.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading joined user data\n"
     ]
    }
   ],
   "source": [
    "train_g_truth = load_golden_truth(train_g_truth_file)\n",
    "test_g_truth = load_golden_truth(test_g_truth_file, test_collection=True)\n",
    "\n",
    "train_users = load_user_data(train_users_file, train_g_truth)\n",
    "test_users = load_user_data(test_users_file, test_g_truth, test_collection=True)\n",
    "logger(\"Loading joined user data\")\n",
    "#train_users = load_joined_user_data(train_users_file, train_g_truth)\n",
    "#test_users = load_joined_user_data(test_users_file, test_g_truth, test_collection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_users = pd.DataFrame(train_users)\n",
    "test_users = pd.DataFrame(test_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users[\"clean_text\"] = train_users[\"text\"].apply(preprocess_text)\n",
    "test_users[\"clean_text\"] = test_users[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users[\"clean_text\"] = train_users[\"clean_text\"].apply(preprocess_text_v2)\n",
    "test_users[\"clean_text\"] = test_users[\"clean_text\"].apply(preprocess_text_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users[\"tokens\"] = train_users[\"clean_text\"].apply(tokenize_text)\n",
    "test_users[\"tokens\"] = test_users[\"clean_text\"].apply(tokenize_text)\n",
    "train_users[\"pos_tags\"] = train_users[\"tokens\"].apply(pos_tag_text)\n",
    "test_users[\"pos_tags\"] = test_users[\"tokens\"].apply(pos_tag_text)\n",
    "train_users[\"stems\"] = train_users[\"tokens\"].apply(stemmize_text)\n",
    "test_users[\"stems\"] = test_users[\"tokens\"].apply(stemmize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_users[\"clean_text\"] \n",
    "X_test = test_users[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_users[\"g_truth\"])\n",
    "y_test = np.array(test_users[\"g_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 299), (1, 299)]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "if oversample:\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(train_users, y_train)\n",
    "    from collections import Counter\n",
    "\n",
    "    print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if oversample:\n",
    "    train_users = X_resampled\n",
    "    y_train = y_resampled\n",
    "    X_train = X_resampled[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if oversample:\n",
    "    save_pickle(\"pickles\", \"X_train_over.pkl\", X_train)\n",
    "    save_pickle(\"pickles\", \"X_test.pkl\", X_test)\n",
    "    save_pickle(\"pickles\", \"train_users_over.pkl\", train_users)\n",
    "    save_pickle(\"pickles\", \"test_users.pkl\", test_users)\n",
    "    save_pickle(\"pickles\", \"y_train_over.pkl\", y_train)\n",
    "    save_pickle(\"pickles\", \"y_test.pkl\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not oversample:\n",
    "    save_pickle(\"pickles\", \"X_train.pkl\", X_train)\n",
    "    save_pickle(\"pickles\", \"X_test.pkl\", X_test)\n",
    "    save_pickle(\"pickles\", \"train_users.pkl\", train_users)\n",
    "    save_pickle(\"pickles\", \"test_users.pkl\", test_users)\n",
    "    save_pickle(\"pickles\", \"y_train.pkl\", y_train)\n",
    "    save_pickle(\"pickles\", \"y_test.pkl\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "logger(\"Finished preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
