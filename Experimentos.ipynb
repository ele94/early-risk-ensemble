{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utiles\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#pickle_path = \"/datos/ecampillo/jupyter/dl-notebooks/pickles\"\n",
    "pickle_path = \"/datos/ecampillo/jupyter/dl-notebooks/newensemble/early-risk-ensemble/pickles\"\n",
    "\n",
    "def logger(message, debug_file=\"log.txt\"):\n",
    "    print(message)\n",
    "    original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "    with open(debug_file, 'a') as f:\n",
    "        sys.stdout = f # Change the standard output to the file we created.\n",
    "        print(message)\n",
    "        sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "        \n",
    "def save_pickle(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with open(file, 'wb') as data_file:\n",
    "        pickle.dump(data, data_file)\n",
    "        \n",
    "def load_pickle(filepath, filename):\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with open(file, 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "    return data\n",
    "\n",
    "def load_nssi_corpus():\n",
    "\n",
    "    with open(\"/datos/erisk/ml/data/nssicorpus.txt\", 'r') as file:\n",
    "        nssi_corpus_original = file.read()\n",
    "\n",
    "    nssi_corpus = nssi_corpus_original.replace('*', '')\n",
    "    nssi_corpus = nssi_corpus.replace(\"Methods of NSSI\", '')\n",
    "    nssi_corpus = nssi_corpus.replace(\"NSSI Terms\", '')\n",
    "    nssi_corpus = nssi_corpus.replace(\"Instruments Used\", '')\n",
    "    nssi_corpus = nssi_corpus.replace(\"Reasons for NSSI\", '')\n",
    "\n",
    "    keys = [\"methods\", \"terms\", \"instruments\", \"reasons\"]\n",
    "\n",
    "    nssi_corpus = nssi_corpus.split(':')\n",
    "    nssi_corpus.remove('')\n",
    "    nssi_corpus = [corpus.split(\"\\n\") for corpus in nssi_corpus]\n",
    "    new_nssi_corpus = {}\n",
    "    for idx, corpus in enumerate(nssi_corpus):\n",
    "        new_list = [word for word in corpus if word != \"\"]\n",
    "        new_nssi_corpus[keys[idx]] = new_list\n",
    "\n",
    "    return new_nssi_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = load_pickle(pickle_path, \"train_users.pkl\")\n",
    "test_users = load_pickle(pickle_path, \"test_users.pkl\")\n",
    "X_train = train_users[\"clean_text\"]\n",
    "X_test = test_users[\"clean_text\"]\n",
    "y_train = load_pickle(pickle_path, \"y_train.pkl\")\n",
    "y_test = load_pickle(pickle_path, \"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feats_train = pd.DataFrame()\n",
    "feats_test = pd.DataFrame()\n",
    "#text len\n",
    "feats_train['char_count'] = X_train.map(len)\n",
    "feats_test['char_count'] = X_test.map(len)\n",
    "#word count\n",
    "feats_train['word_count'] = X_train.map(lambda x: len(x.split()))\n",
    "feats_test['word_count'] = X_test.map(lambda x: len(x.split()))\n",
    "\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#special features\n",
    "#first prons\n",
    "reg = r'\\bI\\b|\\bme\\b|\\bmine\\b|\\bmy\\b|\\bmyself\\b'\n",
    "feats_train['first_prons'] = X_train.map(lambda x: len(re.findall(reg, x)))\n",
    "feats_test['first_prons'] = X_test.map(lambda x: len(re.findall(reg, x)))\n",
    "# sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "feats_train['sentiment'] = X_train.map(lambda x: round(sid.polarity_scores(x)['compound'], 2))\n",
    "feats_test['sentiment'] = X_test.map(lambda x: round(sid.polarity_scores(x)['compound'], 2))\n",
    "\n",
    "nssi_corpus = load_nssi_corpus()\n",
    "\n",
    "# nssi dictionary\n",
    "for key, values in nssi_corpus.items():\n",
    "    feats_train[key] = train_users['stems'].map(lambda x: sum((' '.join(x)).count(word) for word in values))\n",
    "    feats_test[key] = test_users['stems'].map(lambda x: sum((' '.join(x)).count(word) for word in values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_exceptions = ['char_count', 'word_density']\n",
    "text_length = feats_train[\"char_count\"]\n",
    "#text_length = X_train.map(len)\n",
    "\n",
    "norm_feats_train = pd.DataFrame()\n",
    "norm_feats_test = pd.DataFrame()\n",
    "\n",
    "for feature in feats_train.columns:\n",
    "    if feature not in normalize_exceptions:\n",
    "        norm_feats_train[feature] = feats_train[feature] / text_length\n",
    "\n",
    "text_length = feats_test[\"char_count\"]\n",
    "#text_length = X_test.map(len)\n",
    "\n",
    "for feature in feats_test.columns:\n",
    "    if feature not in normalize_exceptions:\n",
    "        norm_feats_test[feature] = feats_test[feature] / text_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize_features(train_feats, test_feats, size=3, strategy='uniform'):\n",
    "    est = KBinsDiscretizer(n_bins=size, encode=encode, strategy=strategy)\n",
    "    train = est.fit_transform(train_feats)\n",
    "    test = est.transform(test_feats)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "logger(\"Discretizing features\")\n",
    "dis_feats_train, dis_feats_test = discretize_features(feats_train, feats_test, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutar para escoger los features no normalizados\n",
    "logger(\"Saving non-normalized features\")\n",
    "feats_train_save = feats_train.values\n",
    "feats_test_save = feats_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(pickle_path, \"feats_train.pkl\", feats_train_save)\n",
    "save_pickle(pickle_path, \"feats_test.pkl\", feats_test_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(exclude_feats=[], normalize=False, discretize=False):\n",
    "    feats_train_ret = feats_train\n",
    "    feats_test_ret = feats_test\n",
    "    if normalize:\n",
    "        feats_train_ret = norm_feats_train\n",
    "        feats_test_ret = norm_feats_test\n",
    "    if discretize:\n",
    "        feats_train_ret, feats_test_ret = discretize_features(feats_train_ret, feats_test_ret)\n",
    "        \n",
    "    for feat in exclude_feats:\n",
    "        feats_train_ret.drop(feat, inplace=True, axis=1)\n",
    "        feats_test_ret.drop(feat, inplace=True, axis=1)\n",
    "    \n",
    "    return feats_train_ret.values, feats_test_ret.values\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL preprocc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = load_pickle(pickle_path, \"embedding_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cnn_model(loc_input_len):\n",
    "    meta_input = Input(shape=(loc_input_len,))\n",
    "    nlp_input = Input(shape=(maxlen,))\n",
    "    emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "    nlp_out = Conv1D(64, 5, activation='relu')(emb)\n",
    "    max_pool = GlobalMaxPooling1D()(nlp_out)\n",
    "    concat = concatenate([max_pool, meta_input])\n",
    "    classifier = Dense(32, activation='relu')(concat)\n",
    "    output = Dense(1, activation='sigmoid')(classifier)\n",
    "    model_cnn = Model(inputs=[nlp_input, meta_input], outputs=[output])\n",
    "\n",
    "    model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_lstm_model(loc_input_len):\n",
    "    meta_input = Input(shape=(loc_input_len,))\n",
    "    nlp_input = Input(shape=(maxlen,)) \n",
    "    emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "    nlp_out = Bidirectional(LSTM(128))(emb)\n",
    "    concat = concatenate([nlp_out, meta_input])\n",
    "    classifier = Dense(32, activation='relu')(concat)\n",
    "    output = Dense(1, activation='sigmoid')(classifier)\n",
    "    model_lstm = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "\n",
    "    model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_to_train, X_train, feats_train, y_train):\n",
    "    history = model_to_train.fit([X_train, feats_train], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    score = model.evaluate([X_test, feats_test], y_test, verbose=1)\n",
    "    logger(\"Test Score: {}\".format(score[0]))\n",
    "    logger(\"Test Accuracy: {}\".format(score[1]))\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    y_pred = model.predict([X_test, feats_test], batch_size=2, verbose=1)\n",
    "    if y_pred.shape[-1] > 1:\n",
    "        y_pred_label = y_pred.argmax(axis=-1)\n",
    "    else:\n",
    "        print(\"Entered here\")\n",
    "        y_pred_label = (y_pred > 0.5).astype('int32')\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    logger(classification_report(y_test, y_pred_label))\n",
    "    logger(confusion_matrix(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 1: CNN, First person, sentiment analysis y nssi dictionary sin normalizar ni discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features()\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 2: CNN, First person, sentiment analysis y nssi dictionary con normalizar ni discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(normalize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 3: CNN, First person, sentiment analysis y nssi dictionary sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(discretize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 4: CNN, First person, sentiment analysis y nssi dictionary sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(discretize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 5: CNN, First person, sentiment analysis y nssi dictionary con normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(normalize=True, discretize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solo first person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 6: CNN, First person sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"sentiment\", \"methods\", \n",
    "                                                                 \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  discretize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 7: CNN, First person con normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"sentiment\", \"methods\", \n",
    "                                                                 \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  discretize=True, normalize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First person y sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 8: CNN, First person y sentiment sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"methods\", \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  discretize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 8: CNN, First person y sentiment con normalizar sin discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"methods\", \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  normalize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 9: CNN, First person y sentiment con normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"methods\", \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  normalize=True, discretize=True)\n",
    "model = define_cnn_model(len(feats_train_new[1,]))\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, feats_train_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
