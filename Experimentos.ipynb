{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments commit 80ac88c\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiments commit 80ac88c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend\n",
    "import tensorflow.keras.models as models\n",
    "from nltk.corpus import stopwords\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utiles\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#pickle_path = \"/datos/ecampillo/jupyter/dl-notebooks/pickles\"\n",
    "pickle_path = \"/datos/ecampillo/jupyter/dl-notebooks/newensemble/early-risk-ensemble/pickles\"\n",
    "\n",
    "def logger(message, debug_file=\"log.txt\"):\n",
    "    print(message)\n",
    "    original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "    with open(debug_file, 'a') as f:\n",
    "        sys.stdout = f # Change the standard output to the file we created.\n",
    "        print(message)\n",
    "        sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "        \n",
    "def save_pickle(filepath, filename, data):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with open(file, 'wb') as data_file:\n",
    "        pickle.dump(data, data_file)\n",
    "        \n",
    "def load_pickle(filepath, filename):\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with open(file, 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 42\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tensorflow.random.set_seed(42) \n",
    "\n",
    "logger(\"Initialized numpy random and tensorflow random seed at 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_users = load_pickle(pickle_path, \"train_users.pkl\")\n",
    "#test_users = load_pickle(pickle_path, \"test_users.pkl\")\n",
    "X_train = load_pickle(pickle_path, \"X_train_over.pkl\")\n",
    "X_test = load_pickle(pickle_path, \"X_test.pkl\")\n",
    "y_train = load_pickle(pickle_path, \"y_train_over.pkl\")\n",
    "y_test = load_pickle(pickle_path, \"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train = load_pickle(pickle_path, \"feats_train_original.pkl\")\n",
    "feats_test = load_pickle(pickle_path, \"feats_test_original.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_exceptions = ['char_count', 'word_density']\n",
    "\n",
    "def normalize_features(feats):\n",
    "    text_length = feats[\"char_count\"]\n",
    "    \n",
    "    norm_feats = pd.DataFrame()\n",
    "    for feature in feats.columns:\n",
    "        if feature not in normalize_exceptions:\n",
    "            norm_feats[feature] = feats[feature] / text_length\n",
    "            \n",
    "    return norm_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize_features(train_feats, test_feats, size=10, strategy='kmeans', encode='onehot-dense'):\n",
    "    est = KBinsDiscretizer(n_bins=size, encode=encode, strategy=strategy)\n",
    "    train = est.fit_transform(train_feats)\n",
    "    test = est.transform(test_feats)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(exclude_feats=[], normalize=False, discretize=False, discretize_size=10):\n",
    "    feats_train_ret = feats_train.copy()\n",
    "    feats_test_ret = feats_test.copy()\n",
    "    \n",
    "    for feat in exclude_feats:\n",
    "        feats_train_ret.drop(feat, inplace=True, axis=1)\n",
    "        feats_test_ret.drop(feat, inplace=True, axis=1)\n",
    "    \n",
    "    if normalize:\n",
    "        feats_train_ret = normalize_features(feats_train_ret)\n",
    "        feats_test_ret = normalize_features(feats_test_ret)\n",
    "        \n",
    "    if discretize:\n",
    "        feats_train_ret, feats_test_ret = discretize_features(feats_train_ret, feats_test_ret, size=discretize_size)\n",
    "    else:\n",
    "        feats_train_ret = feats_train_ret.values\n",
    "        feats_test_ret = feats_test_ret.values\n",
    "    \n",
    "    return feats_train_ret, feats_test_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL preprocc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=50000) # 5000\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 50000  # podria ser la que quisieramos  # antes tenia 10000, voy a probar con 50000\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the GloVe embeddings\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('/datos/erisk/deep-learning/embeddings/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an embedding matrix\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(pickle_path, \"embedding_matrix.pkl\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = load_pickle(pickle_path, \"embedding_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cnn_model(loc_input_len):\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    meta_input = Input(shape=(loc_input_len,))\n",
    "    nlp_input = Input(shape=(maxlen,))\n",
    "    emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "    nlp_out = Conv1D(64, 5, activation='relu')(emb)\n",
    "    max_pool = GlobalMaxPooling1D()(nlp_out)\n",
    "    concat = concatenate([max_pool, meta_input])\n",
    "    classifier = Dense(32, activation='relu')(concat)\n",
    "    output = Dense(1, activation='sigmoid')(classifier)\n",
    "        \n",
    "    return Model(inputs=[nlp_input, meta_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_lstm_model(loc_input_len):\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    meta_input = Input(shape=(loc_input_len,))\n",
    "    nlp_input = Input(shape=(maxlen,)) \n",
    "    emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "    nlp_out = Bidirectional(LSTM(128))(emb)\n",
    "    concat = concatenate([nlp_out, meta_input])\n",
    "    classifier = Dense(32, activation='relu')(concat)\n",
    "    output = Dense(1, activation='sigmoid')(classifier)\n",
    "    \n",
    "    return Model(inputs=[nlp_input , meta_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_to_train, X_train, feats_train, y_train):\n",
    "    history = model_to_train.fit([X_train, feats_train], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_eval, X_test, feats_test, y_test):\n",
    "    score = model_eval.evaluate([X_test, feats_test], y_test, verbose=1)\n",
    "    logger(\"Test Score: {}\".format(score[0]))\n",
    "    logger(\"Test Accuracy: {}\".format(score[1]))\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    y_pred = model_eval.predict([X_test, feats_test], batch_size=2, verbose=1)\n",
    "    if y_pred.shape[-1] > 1:\n",
    "        y_pred_label = y_pred.argmax(axis=-1)\n",
    "    else:\n",
    "        print(\"Entered here\")\n",
    "        y_pred_label = (y_pred > 0.5).astype('int32')\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    logger(classification_report(y_test, y_pred_label))\n",
    "    logger(confusion_matrix(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model\n"
     ]
    }
   ],
   "source": [
    "model = \"\"\n",
    "if model is not None:\n",
    "    print(\"Deleting model\")\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1: CNN, First person, sentiment analysis y nssi dictionary sin normalizar ni discretizar\n",
      "8\n",
      "Evaluating before training\n",
      "14/14 [==============================] - 12s 820ms/step - loss: 24.8152 - acc: 0.7541\n",
      "Test Score: 24.81515884399414\n",
      "Test Accuracy: 0.7541370987892151\n",
      "212/212 [==============================] - 8s 35ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       319\n",
      "           1       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.75       423\n",
      "   macro avg       0.38      0.50      0.43       423\n",
      "weighted avg       0.57      0.75      0.65       423\n",
      "\n",
      "[[319   0]\n",
      " [104   0]]\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "239/239 [==============================] - 65s 269ms/step - loss: 60.3041 - acc: 0.5837 - val_loss: 205.0491 - val_acc: 0.0083\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 88s 369ms/step - loss: 35.8358 - acc: 0.6569 - val_loss: 21.0317 - val_acc: 0.6250\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 77s 320ms/step - loss: 30.1783 - acc: 0.6987 - val_loss: 58.4560 - val_acc: 0.2333\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 75s 316ms/step - loss: 32.1477 - acc: 0.7113 - val_loss: 2.6565e-19 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 69s 289ms/step - loss: 50.0798 - acc: 0.6820 - val_loss: 11.4584 - val_acc: 0.8083\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 73s 304ms/step - loss: 66.7952 - acc: 0.7238 - val_loss: 551.0635 - val_acc: 0.2417\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 73s 305ms/step - loss: 54.0123 - acc: 0.7782 - val_loss: 4.2690 - val_acc: 0.9167\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 63s 265ms/step - loss: 37.4986 - acc: 0.8682 - val_loss: 2.2008e-24 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 59s 246ms/step - loss: 14.8271 - acc: 0.8933 - val_loss: 5.2875e-12 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 58s 242ms/step - loss: 17.4099 - acc: 0.8766 - val_loss: 4.2493e-15 - val_acc: 1.0000\n",
      "Evaluating\n",
      "14/14 [==============================] - 9s 649ms/step - loss: 19.3080 - acc: 0.5721\n",
      "Test Score: 19.30797004699707\n",
      "Test Accuracy: 0.5721040368080139\n",
      "212/212 [==============================] - 7s 34ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.46      0.62       319\n",
      "           1       0.36      0.92      0.51       104\n",
      "\n",
      "    accuracy                           0.57       423\n",
      "   macro avg       0.65      0.69      0.57       423\n",
      "weighted avg       0.80      0.57      0.59       423\n",
      "\n",
      "[[146 173]\n",
      " [  8  96]]\n"
     ]
    }
   ],
   "source": [
    "logger(\"Experiment 1: CNN, First person, sentiment analysis y nssi dictionary sin normalizar ni discretizar\")\n",
    "\n",
    "# selecting features\n",
    "tensorflow.keras.backend.clear_session()\n",
    "train_feats_new, test_feats_new = select_features()\n",
    "logger(len(train_feats_new[1,]))\n",
    "model = define_cnn_model(len(train_feats_new[1,]))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = model.fit([X_train, train_feats_new], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 1: CNN, First person, sentiment analysis y nssi dictionary sin normalizar ni discretizar\")\n",
    "\n",
    "# selecting features\n",
    "\n",
    "train_feats_new, test_feats_new = select_features()\n",
    "logger(len(train_feats_new[1,]))\n",
    "model = define_cnn_model(len(train_feats_new[1,]))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 2: CNN, First person, sentiment analysis y nssi dictionary con normalizar ni discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(normalize=True)\n",
    "logger(len(train_feats_new[1,]))\n",
    "model = define_cnn_model(len(train_feats_new[1,]))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(model.summary())\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 3: CNN, First person, sentiment analysis y nssi dictionary sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(discretize=True)\n",
    "model = define_cnn_model(len(train_feats_new[1,]))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 4: CNN, First person, sentiment analysis y nssi dictionary sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "tensorflow.keras.backend.clear_session()\n",
    "train_feats_new, test_feats_new = select_features(discretize=True)\n",
    "model = define_cnn_model(len(train_feats_new[1,]))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 5: CNN, First person, sentiment analysis y nssi dictionary con normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(normalize=True, discretize=True)\n",
    "model = define_cnn_model(len(train_feats_new[1,]))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solo first person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 6.0: CNN, First person sin normalizar sin discretizar\n",
      "3\n",
      "Evaluating before training\n",
      "14/14 [==============================] - 7s 470ms/step - loss: 194.6896 - acc: 0.7541\n",
      "Test Score: 194.68960571289062\n",
      "Test Accuracy: 0.7541370987892151\n",
      "212/212 [==============================] - 7s 33ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       319\n",
      "           1       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.75       423\n",
      "   macro avg       0.38      0.50      0.43       423\n",
      "weighted avg       0.57      0.75      0.65       423\n",
      "\n",
      "[[319   0]\n",
      " [104   0]]\n",
      "Training\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 59s 243ms/step - loss: 58.2047 - acc: 0.5816 - val_loss: 371.6874 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 66s 278ms/step - loss: 111.5344 - acc: 0.5983 - val_loss: 278.9637 - val_acc: 0.0083\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 68s 287ms/step - loss: 86.1810 - acc: 0.6234 - val_loss: 1.0176e-09 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 67s 279ms/step - loss: 40.7051 - acc: 0.7071 - val_loss: 41.3716 - val_acc: 0.6167\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 67s 280ms/step - loss: 33.9198 - acc: 0.7741 - val_loss: 98.1848 - val_acc: 0.1917\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 67s 280ms/step - loss: 94.5895 - acc: 0.7176 - val_loss: 637.7128 - val_acc: 0.2417\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 68s 285ms/step - loss: 72.6668 - acc: 0.7636 - val_loss: 2.9642e-08 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 66s 278ms/step - loss: 44.7284 - acc: 0.8473 - val_loss: 133.1592 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 59s 248ms/step - loss: 36.8276 - acc: 0.8556 - val_loss: 4.4636e-19 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 57s 240ms/step - loss: 45.7362 - acc: 0.7908 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Evaluating\n",
      "14/14 [==============================] - 6s 455ms/step - loss: 36.8123 - acc: 0.5721\n",
      "Test Score: 36.81229019165039\n",
      "Test Accuracy: 0.5721040368080139\n",
      "212/212 [==============================] - 7s 32ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.48      0.63       319\n",
      "           1       0.35      0.86      0.50       104\n",
      "\n",
      "    accuracy                           0.57       423\n",
      "   macro avg       0.63      0.67      0.56       423\n",
      "weighted avg       0.77      0.57      0.60       423\n",
      "\n",
      "[[153 166]\n",
      " [ 15  89]]\n"
     ]
    }
   ],
   "source": [
    "logger(\"Experiment 6.0: CNN, First person sin normalizar sin discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"sentiment\", \"methods\", \n",
    "                                                                 \"terms\", \"instruments\", \"reasons\"])\n",
    "logger(len(train_feats_new[1,]))\n",
    "model = define_cnn_model(train_feats_new[1,].shape[0])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "#history = train_model(model, X_train, train_feats_new, y_train)\n",
    "history = model.fit([X_train, train_feats_new], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 6: CNN, First person sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"sentiment\", \"methods\", \n",
    "                                                                 \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  discretize=True)\n",
    "model = define_cnn_model(train_feats_new[1,].shape[0])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 7: CNN, First person con normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"sentiment\", \"methods\", \n",
    "                                                                 \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  discretize=True, normalize=True)\n",
    "model = define_cnn_model(train_feats_new[1,].shape[0])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First person y sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 8: CNN, First person y sentiment sin normalizar con discretizar\n",
      "Evaluating before training\n",
      "14/14 [==============================] - 7s 447ms/step - loss: 0.5847 - acc: 0.7541\n",
      "Test Score: 0.5847447514533997\n",
      "Test Accuracy: 0.7541370987892151\n",
      "212/212 [==============================] - 7s 32ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       319\n",
      "           1       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.75       423\n",
      "   macro avg       0.38      0.50      0.43       423\n",
      "weighted avg       0.57      0.75      0.65       423\n",
      "\n",
      "[[319   0]\n",
      " [104   0]]\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "239/239 [==============================] - 58s 240ms/step - loss: 0.4088 - acc: 0.8243 - val_loss: 0.5377 - val_acc: 0.7917\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 69s 288ms/step - loss: 0.0917 - acc: 0.9812 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 71s 297ms/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 70s 293ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 68s 282ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 71s 297ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 77s 323ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 71s 297ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 61s 254ms/step - loss: 7.3868e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 64s 266ms/step - loss: 5.5818e-04 - acc: 1.0000 - val_loss: 7.8357e-04 - val_acc: 1.0000\n",
      "Evaluating\n",
      "14/14 [==============================] - 9s 628ms/step - loss: 0.5131 - acc: 0.8392\n",
      "Test Score: 0.5130662322044373\n",
      "Test Accuracy: 0.839243471622467\n",
      "212/212 [==============================] - 6s 28ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90       319\n",
      "           1       0.91      0.38      0.54       104\n",
      "\n",
      "    accuracy                           0.84       423\n",
      "   macro avg       0.87      0.69      0.72       423\n",
      "weighted avg       0.85      0.84      0.81       423\n",
      "\n",
      "[[315   4]\n",
      " [ 64  40]]\n"
     ]
    }
   ],
   "source": [
    "logger(\"Experiment 8: CNN, First person y sentiment sin normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"methods\", \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  discretize=True)\n",
    "model = define_cnn_model(train_feats_new[1,].shape[0])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 8: CNN, First person y sentiment con normalizar sin discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"methods\", \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  normalize=True)\n",
    "model = define_cnn_model(train_feats_new[1,].shape[0])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger(\"Experiment 9: CNN, First person y sentiment con normalizar con discretizar\")\n",
    "\n",
    "# selecting features\n",
    "train_feats_new, test_feats_new = select_features(exclude_feats=[\"methods\", \"terms\", \"instruments\", \"reasons\"],\n",
    "                                                  normalize=True, discretize=True)\n",
    "model = define_cnn_model(train_feats_new[1,].shape[0])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "logger(\"Evaluating before training\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)\n",
    "logger(\"Training\")\n",
    "history = train_model(model, X_train, train_feats_new, y_train)\n",
    "logger(\"Evaluating\")\n",
    "evaluate_model(model, X_test, test_feats_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
