{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from preprocessing import preprocess\n",
    "from windowfy import windowfy\n",
    "from featurizing import featurize\n",
    "from training import train, do_ensemble, do_train\n",
    "from eval_erisk import evaluate\n",
    "from IPython.display import display, Markdown\n",
    "from itertools import product\n",
    "from numpy.random import seed\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 42\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "tensorflow.random.set_seed(42) \n",
    "logger(\"Initialized numpy random and tensorflow random seed at 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With max_size = 10 and new data, sample_weights=10, oversample False, include_new_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowfying training users\n",
      "[====================] 100%\n",
      "Windowfying test users\n",
      "[====================] 100%Data size: 5424\n",
      "\n",
      "Finished windowfying\n",
      "Featurizing calculate_feats=True, normalize=False, discretize=True, discretize_size=10, include_feats=['first_prons', 'sentiment', 'nssi']\n",
      "Initialized numpy random and tensorflow random seed at 42\n",
      "Data size: 5424, 5424\n",
      "Data size: 4650, 4650\n",
      "Calculating first prons\n",
      "Calculating sentiment\n",
      "Calculating NSSI words\n",
      "Calculating first prons\n",
      "Calculating sentiment\n",
      "Calculating NSSI words\n",
      "Discretizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:188: UserWarning: Feature 5 is constant and will be replaced with 0.\n",
      "  \"replaced with 0.\" % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:209: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    }
   ],
   "source": [
    "train_users, y_train, test_users, y_test, train_samples, X_train, X_test = windowfy(window_size=10, max_size=20, sample_weights_size=20, is_oversample=False, include_new_data=True)\n",
    "\n",
    "feats_train, feats_test = featurize(calculate_feats=True, include_feats=[\"first_prons\",\"sentiment\",\"nssi\"], \n",
    "                       train_users=train_users, test_users=test_users, discretize=True)\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with cnn_model=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 6s 26ms/step - loss: 0.4837 - tp: 215.0000 - fp: 40.0000 - tn: 3562.0000 - fn: 833.0000 - accuracy: 0.8123 - precision: 0.8431 - recall: 0.2052 - f1_metric: 0.1597\n",
      "Test Score: 0.4836678206920624\n",
      "Test Accuracy: 215.0\n",
      "146/146 [==============================] - 3s 21ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89      3602\n",
      "           1       0.84      0.21      0.33      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.83      0.60      0.61      4650\n",
      "weighted avg       0.82      0.81      0.76      4650\n",
      "\n",
      "[[3562   40]\n",
      " [ 833  215]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.7735849056603774, 'recall': 0.3942307692307692, 'F1': 0.5222929936305732, 'ERDE_5': 0.2527022353428698, 'ERDE_50': 0.15591100380598058, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5019338878990808}\n",
      "Starting training with cnn_model=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00040: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 7s 25ms/step - loss: 0.5128 - tp: 137.0000 - fp: 28.0000 - tn: 3574.0000 - fn: 911.0000 - accuracy: 0.7981 - precision: 0.8303 - recall: 0.1307 - f1_metric: 0.1080\n",
      "Test Score: 0.5127513408660889\n",
      "Test Accuracy: 137.0\n",
      "146/146 [==============================] - 3s 20ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      3602\n",
      "           1       0.83      0.13      0.23      1048\n",
      "\n",
      "    accuracy                           0.80      4650\n",
      "   macro avg       0.81      0.56      0.55      4650\n",
      "weighted avg       0.80      0.80      0.74      4650\n",
      "\n",
      "[[3574   28]\n",
      " [ 911  137]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.8048780487804879, 'recall': 0.3173076923076923, 'F1': 0.45517241379310347, 'ERDE_5': 0.2504203410714821, 'ERDE_50': 0.1724985888257367, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.43565748759454104}\n",
      "Starting training with cnn_model=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 6s 25ms/step - loss: 0.4540 - tp: 309.0000 - fp: 171.0000 - tn: 3431.0000 - fn: 739.0000 - accuracy: 0.8043 - precision: 0.6438 - recall: 0.2948 - f1_metric: 0.2096\n",
      "Test Score: 0.45403289794921875\n",
      "Test Accuracy: 309.0\n",
      "146/146 [==============================] - 3s 22ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88      3602\n",
      "           1       0.64      0.29      0.40      1048\n",
      "\n",
      "    accuracy                           0.80      4650\n",
      "   macro avg       0.73      0.62      0.64      4650\n",
      "weighted avg       0.78      0.80      0.78      4650\n",
      "\n",
      "[[3431  171]\n",
      " [ 739  309]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5777777777777777, 'recall': 0.5, 'F1': 0.5360824742268041, 'ERDE_5': 0.26772458803215166, 'ERDE_50': 0.14501841512555347, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5151858512456746}\n",
      "Starting training with cnn_model=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 4s 20ms/step - loss: 0.4398 - tp: 228.0000 - fp: 35.0000 - tn: 3567.0000 - fn: 820.0000 - accuracy: 0.8161 - precision: 0.8669 - recall: 0.2176 - f1_metric: 0.1742\n",
      "Test Score: 0.43980520963668823\n",
      "Test Accuracy: 228.0\n",
      "146/146 [==============================] - 2s 16ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89      3602\n",
      "           1       0.87      0.22      0.35      1048\n",
      "\n",
      "    accuracy                           0.82      4650\n",
      "   macro avg       0.84      0.60      0.62      4650\n",
      "weighted avg       0.83      0.82      0.77      4650\n",
      "\n",
      "[[3567   35]\n",
      " [ 820  228]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.7678571428571429, 'recall': 0.41346153846153844, 'F1': 0.5375, 'ERDE_5': 0.2532453426741242, 'ERDE_50': 0.1517641075510402, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5165481215254107}\n",
      "Starting training with cnn_model=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 4s 19ms/step - loss: 0.4803 - tp: 256.0000 - fp: 56.0000 - tn: 3546.0000 - fn: 792.0000 - accuracy: 0.8176 - precision: 0.8205 - recall: 0.2443 - f1_metric: 0.1844\n",
      "Test Score: 0.4803161323070526\n",
      "Test Accuracy: 256.0\n",
      "146/146 [==============================] - 3s 16ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89      3602\n",
      "           1       0.82      0.24      0.38      1048\n",
      "\n",
      "    accuracy                           0.82      4650\n",
      "   macro avg       0.82      0.61      0.63      4650\n",
      "weighted avg       0.82      0.82      0.78      4650\n",
      "\n",
      "[[3546   56]\n",
      " [ 792  256]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.746031746031746, 'recall': 0.4519230769230769, 'F1': 0.5628742514970061, 'ERDE_5': 0.25499347862273763, 'ERDE_50': 0.14405155117393084, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5409332786340467}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.746031746031746,\n",
       " 'recall': 0.4519230769230769,\n",
       " 'F1': 0.5628742514970061,\n",
       " 'ERDE_5': 0.25499347862273763,\n",
       " 'ERDE_50': 0.14405155117393084,\n",
       " 'median_latency_tps': 11.0,\n",
       " 'median_penalty_tps': 0.03898023902249159,\n",
       " 'speed': 0.9610197609775084,\n",
       " 'latency_weighted_f1': 0.5409332786340467}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = do_train(model_name=\"cnn_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=30, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)\n",
    "y_pred = do_train(model_name=\"cnn_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=30, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)\n",
    "y_pred = do_train(model_name=\"cnn_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=30, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)\n",
    "y_pred = do_train(model_name=\"cnn_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=30, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)\n",
    "y_pred = do_train(model_name=\"cnn_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=30, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with cnn_model=lstm_model_32 and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 34s 221ms/step - loss: 0.4437 - tp: 345.0000 - fp: 178.0000 - tn: 3424.0000 - fn: 703.0000 - accuracy: 0.8105 - precision: 0.6597 - recall: 0.3292 - f1_metric: 0.2309\n",
      "Test Score: 0.44370993971824646\n",
      "Test Accuracy: 345.0\n",
      "146/146 [==============================] - 33s 221ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89      3602\n",
      "           1       0.66      0.33      0.44      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.74      0.64      0.66      4650\n",
      "weighted avg       0.79      0.81      0.79      4650\n",
      "\n",
      "[[3424  178]\n",
      " [ 703  345]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.58, 'recall': 0.5576923076923077, 'F1': 0.5686274509803922, 'ERDE_5': 0.27001307394087987, 'ERDE_50': 0.13315896249350387, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5464622170264264}\n",
      "Starting training with cnn_model=lstm_model_32 and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 32s 202ms/step - loss: 0.4540 - tp: 396.0000 - fp: 268.0000 - tn: 3334.0000 - fn: 652.0000 - accuracy: 0.8022 - precision: 0.5964 - recall: 0.3779 - f1_metric: 0.2498\n",
      "Test Score: 0.4539654850959778\n",
      "Test Accuracy: 396.0\n",
      "146/146 [==============================] - 31s 203ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      3602\n",
      "           1       0.60      0.38      0.46      1048\n",
      "\n",
      "    accuracy                           0.80      4650\n",
      "   macro avg       0.72      0.65      0.67      4650\n",
      "weighted avg       0.78      0.80      0.78      4650\n",
      "\n",
      "[[3334  268]\n",
      " [ 652  396]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5161290322580645, 'recall': 0.6153846153846154, 'F1': 0.5614035087719298, 'ERDE_5': 0.280473439005424, 'ERDE_50': 0.12943681572020307, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5395198658119346}\n",
      "Starting training with cnn_model=lstm_model_32 and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 5424\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 24s 151ms/step - loss: 0.4447 - tp: 381.0000 - fp: 228.0000 - tn: 3374.0000 - fn: 667.0000 - accuracy: 0.8075 - precision: 0.6256 - recall: 0.3635 - f1_metric: 0.2431\n",
      "Test Score: 0.4446992576122284\n",
      "Test Accuracy: 381.0\n",
      "146/146 [==============================] - 23s 150ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88      3602\n",
      "           1       0.63      0.36      0.46      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.73      0.65      0.67      4650\n",
      "weighted avg       0.79      0.81      0.79      4650\n",
      "\n",
      "[[3374  228]\n",
      " [ 667  381]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5357142857142857, 'recall': 0.5769230769230769, 'F1': 0.5555555555555555, 'ERDE_5': 0.27583024264363804, 'ERDE_50': 0.13424319143347518, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5338998672097268}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5357142857142857,\n",
       " 'recall': 0.5769230769230769,\n",
       " 'F1': 0.5555555555555555,\n",
       " 'ERDE_5': 0.27583024264363804,\n",
       " 'ERDE_50': 0.13424319143347518,\n",
       " 'median_latency_tps': 11.0,\n",
       " 'median_penalty_tps': 0.03898023902249159,\n",
       " 'speed': 0.9610197609775084,\n",
       " 'latency_weighted_f1': 0.5338998672097268}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = do_train(model_name=\"lstm_model_32\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=10, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)\n",
    "y_pred = do_train(model_name=\"lstm_model_32\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=10, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)\n",
    "y_pred = do_train(model_name=\"lstm_model_32\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=10, feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, train_sample_weights=train_samples)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign to variable dense_2/kernel:0 due to variable shape (130, 32) and value shape (72, 32) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c593ad39de92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m y_pred = do_train(model_name=\"ensemble_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n\u001b[1;32m      2\u001b[0m          \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cnn_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lstm_model_32\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeats_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeats_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_users\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datos/ecampillo/jupyter/dl-notebooks/newensemble/early-risk-ensemble/training.py\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(model_name, maxlen, epochs, early_epochs, batch_size, shuffle, patience, model_names, early_stopping, validation_split, feats_train, feats_test, X_train, X_test, y_train, y_test, train_sample_weights, save)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m\"ensemble_model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         y_pred = do_ensemble(maxlen, batch_size, shuffle, model_names, feats_train, feats_test, X_train, X_test,\n\u001b[0;32m---> 42\u001b[0;31m                              y_train, y_test, save)\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         y_pred = train(model_name, maxlen, epochs, early_epochs, batch_size, shuffle, patience, early_stopping, \n",
      "\u001b[0;32m/datos/ecampillo/jupyter/dl-notebooks/newensemble/early-risk-ensemble/training.py\u001b[0m in \u001b[0;36mdo_ensemble\u001b[0;34m(maxlen, batch_size, shuffle, model_names, feats_train, feats_test, X_train, X_test, y_train, y_test, save)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# reload trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/{}{}{}{}.hdf5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# reload trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2324\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2325\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    711\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    712\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3802\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    899\u001b[0m             (\"Cannot assign to variable%s due to variable shape %s and value \"\n\u001b[1;32m    900\u001b[0m              \"shape %s are incompatible\") %\n\u001b[0;32m--> 901\u001b[0;31m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[0m\u001b[1;32m    902\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    903\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign to variable dense_2/kernel:0 due to variable shape (130, 32) and value shape (72, 32) are incompatible"
     ]
    }
   ],
   "source": [
    "y_pred = do_train(model_name=\"ensemble_model\", maxlen=1000, epochs=100, batch_size=batch_size,\n",
    "         shuffle=True, patience=10, model_names=[\"cnn_model\", \"lstm_model_32\"], feats_train=feats_train, feats_test=feats_test, \n",
    "         X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
    "evaluate(1, 10, {\"test\":\"test\"}, y_pred=y_pred, test_users=test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
