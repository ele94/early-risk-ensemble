{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisitos: Preprocessing, featurize\n",
    "\n",
    "En esta libreta cargamos los features textuales, calculamos los embeddings, y entrenamos el modelo de deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. LibrerÃ­as, funciones, random seed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow.keras.models\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tokenizer import tokenizer as reddit_tokenizer\n",
    "# my modules\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 42\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow\n",
    "import sys\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tensorflow.random.set_seed(42) \n",
    "\n",
    "logger(\"Initialized numpy random and tensorflow random seed at 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle(pickle_path, \"X_train.pkl\")\n",
    "X_test = load_pickle(pickle_path, \"X_test.pkl\")\n",
    "y_train = load_pickle(pickle_path, \"y_train.pkl\")\n",
    "y_test = load_pickle(pickle_path, \"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the Embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=50000) # 5000\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 50000  # podria ser la que quisieramos  # antes tenia 10000, voy a probar con 50000\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the GloVe embeddings\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('/datos/erisk/deep-learning/embeddings/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an embedding matrix\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(pickle_path, \"embedding_matrix.pkl\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42,probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train = load_pickle(pickle_path, \"feats_train.pkl\").values\n",
    "feats_test = load_pickle(pickle_path, \"feats_test.pkl\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_input_len = len(feats_train[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM model (takes too long to train)\n",
    "meta_input = Input(shape=(meta_input_len,))\n",
    "nlp_input = Input(shape=(maxlen,)) \n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(128))(emb)\n",
    "concat = concatenate([nlp_out, meta_input])\n",
    "classifier = Dense(32, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(classifier)\n",
    "model_lstm = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model_lstm.fit([X_train, feats_train.values()], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "meta_input = Input(shape=(meta_input_len,))\n",
    "nlp_input = Input(shape=(maxlen,))\n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "nlp_out = Conv1D(64, 5, activation='relu')(emb)\n",
    "max_pool = GlobalMaxPooling1D()(nlp_out)\n",
    "concat = concatenate([max_pool, meta_input])\n",
    "classifier = Dense(32, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(classifier)\n",
    "model_cnn = Model(inputs=[nlp_input, meta_input], outputs=[output])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/136 [======================>.......] - ETA: 10s - loss: 32.5467 - acc: 0.8286"
     ]
    }
   ],
   "source": [
    "#training with feats_train.values()\n",
    "history = model_cnn.fit([X_train, feats_train], y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    score = model.evaluate([X_test, feats_test], y_test, verbose=1)\n",
    "    logger(\"Test Score: {}\".format(score[0]))\n",
    "    logger(\"Test Accuracy: {}\".format(score[1]))\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    y_pred = model.predict([X_test, feats_test], batch_size=2, verbose=1)\n",
    "    if y_pred.shape[-1] > 1:\n",
    "        y_pred_label = y_pred.argmax(axis=-1)\n",
    "    else:\n",
    "        print(\"Entered here\")\n",
    "        y_pred_label = (y_pred > 0.5).astype('int32')\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    logger(classification_report(y_test, y_pred_label))\n",
    "    logger(confusion_matrix(y_test, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model with no text features for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-20c2ab186e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "#CNN model\n",
    "nlp_input = Input(shape=(maxlen,))\n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "nlp_out = Conv1D(64, 5, activation='relu')(emb)\n",
    "max_pool = GlobalMaxPooling1D()(nlp_out)\n",
    "#concat = concatenate([max_pool, meta_input])\n",
    "classifier = Dense(32, activation='relu')(max_pool)\n",
    "output = Dense(1, activation='sigmoid')(classifier)\n",
    "model_cnn_simple = Model(inputs=nlp_input, outputs=output)\n",
    "\n",
    "model_cnn_simple.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "#Bidirectional LSTM model\n",
    "nlp_input = Input(shape=(maxlen,)) \n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(128))(emb)\n",
    "classifier = Dense(32, activation='relu')(nlp_out)\n",
    "output = Dense(1, activation='sigmoid')(classifier)\n",
    "model_lstm = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "\n",
    "model_lstm_simple.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "239/239 [==============================] - 51s 213ms/step - loss: 0.4333 - acc: 0.8054 - val_loss: 0.6103 - val_acc: 0.7250\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 50s 209ms/step - loss: 0.1152 - acc: 0.9749 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "129/239 [===============>..............] - ETA: 22s - loss: 0.0245 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "history = model_cnn_simple.fit(X_train, y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)\n",
    "\n",
    "score = model_cnn_simple.evaluate(X_test, y_test, verbose=1)\n",
    "logger(\"Test Score: {}\".format(score[0]))\n",
    "logger(\"Test Accuracy: {}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_lstm_simple.fit(X_train, y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2, shuffle=True)\n",
    "\n",
    "score = model_cnn_simple.evaluate(X_test, y_test, verbose=1)\n",
    "logger(\"Test Score: {}\".format(score[0]))\n",
    "logger(\"Test Accuracy: {}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_cnn_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_lstm_simple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
