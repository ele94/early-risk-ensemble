{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved as an example of how to do experiments with this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from experiment_utils import *\n",
    "from preprocessing import preprocess\n",
    "from windowfy import windowfy\n",
    "from featurizing import featurize\n",
    "from tfidf_featurizer import combine_features, tfidf_featurize\n",
    "from training import train, do_ensemble, do_train\n",
    "from training_traditional import train_and_evaluate\n",
    "from eval_erisk import evaluate, ensemble_vote\n",
    "from IPython.display import display, Markdown\n",
    "from itertools import product\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment params. this is the part that must be modified for a different experiment\n",
    "\n",
    "seed_part = np.random.randint(1,101,10).tolist()\n",
    "\n",
    "first_part = {\n",
    "    \"random_seed\": seed_part,\n",
    "    \"include_feats\": [[\"first_prons\", \"nssi\"],[\"first_prons\",\"sentiment\",\"nssi\"]],\n",
    "    \"feat_window_size\": [10], #10\n",
    "    \"max_size\": [20],\n",
    "    \"sample_weights_size\": [20],\n",
    "    \"oversample\": [True],\n",
    "    \"include_new_data\": [True],\n",
    "    \"tfidf_max_features\": [5000],\n",
    "    \"scale\": [False],\n",
    "    \"normalize\": [True],\n",
    "    \"discretize\": [True],\n",
    "    \"discretize_size\": [50, 75],\n",
    "    \"dis_strategy\": [\"quantile\"]\n",
    "}\n",
    "\n",
    "second_part = {\n",
    "    \"eval_window_size\": [1],\n",
    "    \"maxlen\": [1000],\n",
    "    \"batch_size\": [32],\n",
    "    \"epochs\": [100],\n",
    "    \"patience\": [10],\n",
    "    \"iterations\": [5],\n",
    "    \"shuffle\": [True],\n",
    "}\n",
    "\n",
    "models = [\"svm\", \"bayes\", \"cnn_model\"]\n",
    "ensemble_combinations = [[\"svm\", \"bayes\", \"cnn_model\"]]\n",
    "weights = [[1, 1, 1], [2, 1, 1], [1, 2, 1], [2, 2, 1], [1, 1, 2], [3, 3, 1], [5, 5, 1], [1, 5, 1]]\n",
    "eval_filename = \"experiments_20-bothdata-seed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 79\n",
      "********** CALCULATING FEATURES FOR {'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'} ***********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculating features for {'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA FOR PARAMS {'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'}\n",
      "Windowfying training users\n",
      "[====================] 100%\n",
      "Windowfying test users\n",
      "[====================] 100%\n",
      "Oversampling train users\n",
      "After oversample: positive messages: 2792, negative messages: 3491\n",
      "Data size: 6283\n",
      "\n",
      "Finished windowfying\n",
      "Featurizing calculate_feats=True, normalize=True, discretize=True, discretize_size=50, include_feats=['first_prons', 'nssi']\n",
      "Data size: 6283, 6283\n",
      "Data size: 4650, 4650\n",
      "Calculating first prons\n",
      "Calculating NSSI words\n",
      "Calculating first prons\n",
      "Calculating NSSI words\n",
      "Normalizing features\n",
      "Discretizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:188: UserWarning: Feature 4 is constant and will be replaced with 0.\n",
      "  \"replaced with 0.\" % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the combined the same from tfidf: False\n",
      "************ STARTING EXPERIMENT {'eval_window_size': 1, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 5, 'shuffle': True, 'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'} ***************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Experiment {'eval_window_size': 1, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 5, 'shuffle': True, 'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING AND EVALUATING TRADITIONAL MODEL svm\n",
      "Starting training traditional\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      3602\n",
      "           1       0.51      0.61      0.55      1048\n",
      "\n",
      "    accuracy                           0.78      4650\n",
      "   macro avg       0.69      0.72      0.70      4650\n",
      "weighted avg       0.80      0.78      0.79      4650\n",
      "\n",
      "[[2981  621]\n",
      " [ 410  638]]\n",
      "Evaluating after getting time 295.090787041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      3602\n",
      "           1       0.51      0.61      0.55      1048\n",
      "\n",
      "    accuracy                           0.78      4650\n",
      "   macro avg       0.69      0.72      0.70      4650\n",
      "weighted avg       0.80      0.78      0.79      4650\n",
      "\n",
      "[[2981  621]\n",
      " [ 410  638]]\n",
      "Evaluated with elapsed time 114.53857639900002\n",
      "EVALUATING FOR WINDOW SIZES 1, 2 AND 3 MODEL svm\n",
      "{'precision': 0.38596491228070173, 'recall': 0.8461538461538461, 'F1': 0.5301204819277108, 'ERDE_5': 0.3268391920315719, 'ERDE_50': 0.11919811768914496, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5094562588314502}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.45930232558139533, 'recall': 0.7596153846153846, 'F1': 0.572463768115942, 'ERDE_5': 0.29979032307015513, 'ERDE_50': 0.11315661519373801, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5479201274039819}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.4857142857142857, 'recall': 0.6538461538461539, 'F1': 0.5573770491803278, 'ERDE_5': 0.28767142138009355, 'ERDE_50': 0.12695538453800798, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5313108309251452}\n",
      "Writing results to CSV file\n",
      "TRAINING AND EVALUATING TRADITIONAL MODEL bayes\n",
      "Starting training traditional\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.82      3602\n",
      "           1       0.45      0.69      0.54      1048\n",
      "\n",
      "    accuracy                           0.74      4650\n",
      "   macro avg       0.67      0.72      0.68      4650\n",
      "weighted avg       0.79      0.74      0.76      4650\n",
      "\n",
      "[[2709  893]\n",
      " [ 322  726]]\n",
      "Evaluating after getting time 417.198390233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.82      3602\n",
      "           1       0.45      0.69      0.54      1048\n",
      "\n",
      "    accuracy                           0.74      4650\n",
      "   macro avg       0.67      0.72      0.68      4650\n",
      "weighted avg       0.79      0.74      0.76      4650\n",
      "\n",
      "[[2709  893]\n",
      " [ 322  726]]\n",
      "Evaluated with elapsed time 2.765099771999985\n",
      "EVALUATING FOR WINDOW SIZES 1, 2 AND 3 MODEL bayes\n",
      "{'precision': 0.3865546218487395, 'recall': 0.8846153846153846, 'F1': 0.5380116959064328, 'ERDE_5': 0.3302709521142193, 'ERDE_50': 0.11322926971033481, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.517039871403104}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.40384615384615385, 'recall': 0.8076923076923077, 'F1': 0.5384615384615384, 'ERDE_5': 0.3177849487889666, 'ERDE_50': 0.11935460434027145, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5153756991940083}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.4301675977653631, 'recall': 0.7403846153846154, 'F1': 0.5441696113074205, 'ERDE_5': 0.3050995973098398, 'ERDE_50': 0.12311587277635756, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5187210502713381}\n",
      "Writing results to CSV file\n",
      "TRAINING AND EVALUATING DL MODEL cnn_model\n",
      "STARTING ITERATION FOR DL MODEL cnn_model FOR 5 ITERATIONS\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 2s 9ms/step - loss: 0.4789 - tp: 398.0000 - fp: 252.0000 - tn: 3350.0000 - fn: 650.0000 - accuracy: 0.8060 - precision: 0.6123 - recall: 0.3798 - f1_metric: 0.2668\n",
      "Test Score: 0.4788718521595001\n",
      "Test Accuracy: 398.0\n",
      "146/146 [==============================] - 1s 7ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      3602\n",
      "           1       0.61      0.38      0.47      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.72      0.65      0.68      4650\n",
      "weighted avg       0.79      0.81      0.79      4650\n",
      "\n",
      "[[3350  252]\n",
      " [ 650  398]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5409836065573771, 'recall': 0.6346153846153846, 'F1': 0.5840707964601771, 'ERDE_5': 0.2781318802245236, 'ERDE_50': 0.12238373880142664, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5613035772081024}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 2s 8ms/step - loss: 0.4716 - tp: 482.0000 - fp: 291.0000 - tn: 3311.0000 - fn: 566.0000 - accuracy: 0.8157 - precision: 0.6235 - recall: 0.4599 - f1_metric: 0.2992\n",
      "Test Score: 0.4716081917285919\n",
      "Test Accuracy: 482.0\n",
      "146/146 [==============================] - 1s 7ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.89      3602\n",
      "           1       0.62      0.46      0.53      1048\n",
      "\n",
      "    accuracy                           0.82      4650\n",
      "   macro avg       0.74      0.69      0.71      4650\n",
      "weighted avg       0.80      0.82      0.81      4650\n",
      "\n",
      "[[3311  291]\n",
      " [ 566  482]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.568, 'recall': 0.6826923076923077, 'F1': 0.6200873362445415, 'ERDE_5': 0.2769674659430725, 'ERDE_50': 0.10940093556662539, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5959161836629091}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 3s 11ms/step - loss: 0.4760 - tp: 449.0000 - fp: 296.0000 - tn: 3306.0000 - fn: 599.0000 - accuracy: 0.8075 - precision: 0.6027 - recall: 0.4284 - f1_metric: 0.2857\n",
      "Test Score: 0.4759669005870819\n",
      "Test Accuracy: 449.0\n",
      "146/146 [==============================] - 1s 9ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88      3602\n",
      "           1       0.60      0.43      0.50      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.72      0.67      0.69      4650\n",
      "weighted avg       0.79      0.81      0.80      4650\n",
      "\n",
      "[[3306  296]\n",
      " [ 599  449]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5365853658536586, 'recall': 0.6346153846153846, 'F1': 0.5814977973568282, 'ERDE_5': 0.2787012835211319, 'ERDE_50': 0.12296497493419281, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5588308742248066}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Evaluating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 3s 11ms/step - loss: 0.4907 - tp: 456.0000 - fp: 370.0000 - tn: 3232.0000 - fn: 592.0000 - accuracy: 0.7931 - precision: 0.5521 - recall: 0.4351 - f1_metric: 0.2825\n",
      "Test Score: 0.490740567445755\n",
      "Test Accuracy: 456.0\n",
      "146/146 [==============================] - 1s 9ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      3602\n",
      "           1       0.55      0.44      0.49      1048\n",
      "\n",
      "    accuracy                           0.79      4650\n",
      "   macro avg       0.70      0.67      0.68      4650\n",
      "weighted avg       0.78      0.79      0.78      4650\n",
      "\n",
      "[[3232  370]\n",
      " [ 592  456]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5174825174825175, 'recall': 0.7115384615384616, 'F1': 0.5991902834008097, 'ERDE_5': 0.2856578493668703, 'ERDE_50': 0.11102727897657819, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5758337029338917}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 2s 10ms/step - loss: 0.4772 - tp: 505.0000 - fp: 393.0000 - tn: 3209.0000 - fn: 543.0000 - accuracy: 0.7987 - precision: 0.5624 - recall: 0.4819 - f1_metric: 0.3085\n",
      "Test Score: 0.47724902629852295\n",
      "Test Accuracy: 505.0\n",
      "146/146 [==============================] - 1s 9ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87      3602\n",
      "           1       0.56      0.48      0.52      1048\n",
      "\n",
      "    accuracy                           0.80      4650\n",
      "   macro avg       0.71      0.69      0.70      4650\n",
      "weighted avg       0.79      0.80      0.79      4650\n",
      "\n",
      "[[3209  393]\n",
      " [ 543  505]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.4797297297297297, 'recall': 0.6826923076923077, 'F1': 0.5634920634920635, 'ERDE_5': 0.2903111994229939, 'ERDE_50': 0.1227693666202804, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5415270081698659}\n",
      "Evaluating for elapsed time\n",
      "146/146 [==============================] - 3s 10ms/step - loss: 0.4772 - tp: 505.0000 - fp: 393.0000 - tn: 3209.0000 - fn: 543.0000 - accuracy: 0.7987 - precision: 0.5624 - recall: 0.4819 - f1_metric: 0.3085\n",
      "Test Score: 0.47724902629852295\n",
      "Test Accuracy: 505.0\n",
      "146/146 [==============================] - 1s 9ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87      3602\n",
      "           1       0.56      0.48      0.52      1048\n",
      "\n",
      "    accuracy                           0.80      4650\n",
      "   macro avg       0.71      0.69      0.70      4650\n",
      "weighted avg       0.79      0.80      0.79      4650\n",
      "\n",
      "[[3209  393]\n",
      " [ 543  505]]\n",
      "Evaluated with elapsed time 50.500592444000176\n",
      "EVALUATING FOR WINDOW SIZES 1, 2 AND 3 MODEL cnn_model\n",
      "{'precision': 0.568, 'recall': 0.6826923076923077, 'F1': 0.6200873362445415, 'ERDE_5': 0.2769674659430725, 'ERDE_50': 0.10940093556662539, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5959161836629091}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.6055045871559633, 'recall': 0.6346153846153846, 'F1': 0.619718309859155, 'ERDE_5': 0.2707683963855716, 'ERDE_50': 0.11482766907544874, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5931486920301263}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5730337078651685, 'recall': 0.49038461538461536, 'F1': 0.5284974093264249, 'ERDE_5': 0.26792340398524084, 'ERDE_50': 0.14738248131941337, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5037817723279875}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [1, 1, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [1, 1, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.42, 'recall': 0.8076923076923077, 'F1': 0.5526315789473685, 'ERDE_5': 0.31289588271012986, 'ERDE_50': 0.11470471527812945, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5310898679086231}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.48484848484848486, 'recall': 0.7692307692307693, 'F1': 0.5947955390334573, 'ERDE_5': 0.2951402343432478, 'ERDE_50': 0.10614265993774298, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5692944526412838}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5036496350364964, 'recall': 0.6634615384615384, 'F1': 0.5726141078838175, 'ERDE_5': 0.28534646598882946, 'ERDE_50': 0.12226637381308297, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5458353154415272}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [2, 1, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [2, 1, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.42857142857142855, 'recall': 0.8076923076923077, 'F1': 0.5599999999999999, 'ERDE_5': 0.31057463491605974, 'ERDE_50': 0.11237977074705904, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5381710661474046}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.484472049689441, 'recall': 0.75, 'F1': 0.5886792452830188, 'ERDE_5': 0.29397950785045274, 'ERDE_50': 0.10970832005991375, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5634403870433686}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5075757575757576, 'recall': 0.6442307692307693, 'F1': 0.5677966101694915, 'ERDE_5': 0.28360333763402074, 'ERDE_50': 0.12525079780248657, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5412431121612833}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [1, 2, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [1, 2, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.4263959390862944, 'recall': 0.8076923076923077, 'F1': 0.5581395348837209, 'ERDE_5': 0.31115772928630997, 'ERDE_50': 0.11296100687982634, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5363831224060512}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.49375, 'recall': 0.7596153846153846, 'F1': 0.5984848484848486, 'ERDE_5': 0.2928181295209788, 'ERDE_50': 0.10618178160052583, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5728255877405267}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5074626865671642, 'recall': 0.6538461538461539, 'F1': 0.5714285714285714, 'ERDE_5': 0.284185038911049, 'ERDE_50': 0.12346796774140195, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5447052216207372}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [2, 2, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [2, 2, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.42, 'recall': 0.8076923076923077, 'F1': 0.5526315789473685, 'ERDE_5': 0.31289588271012986, 'ERDE_50': 0.11470471527812945, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5310898679086231}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.48484848484848486, 'recall': 0.7692307692307693, 'F1': 0.5947955390334573, 'ERDE_5': 0.2951402343432478, 'ERDE_50': 0.10614265993774298, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5692944526412838}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5036496350364964, 'recall': 0.6634615384615384, 'F1': 0.5726141078838175, 'ERDE_5': 0.28534646598882946, 'ERDE_50': 0.12226637381308297, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5458353154415272}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [1, 1, 2]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [1, 1, 2] FOR WINDOW SIZES 1, 2 AND 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.5847457627118644, 'recall': 0.6634615384615384, 'F1': 0.6216216216216216, 'ERDE_5': 0.2740632530733085, 'ERDE_50': 0.11122288729049135, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.597390662229262}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.6237623762376238, 'recall': 0.6057692307692307, 'F1': 0.6146341463414634, 'ERDE_5': 0.26786441033081426, 'ERDE_50': 0.11901368699316878, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5882825054214534}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5975609756097561, 'recall': 0.47115384615384615, 'F1': 0.5268817204301075, 'ERDE_5': 0.26501800613564536, 'ERDE_50': 0.14920443304328185, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5022416425158948}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [3, 3, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [3, 3, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.42, 'recall': 0.8076923076923077, 'F1': 0.5526315789473685, 'ERDE_5': 0.31289588271012986, 'ERDE_50': 0.11470471527812945, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5310898679086231}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.48484848484848486, 'recall': 0.7692307692307693, 'F1': 0.5947955390334573, 'ERDE_5': 0.2951402343432478, 'ERDE_50': 0.10614265993774298, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5692944526412838}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5036496350364964, 'recall': 0.6634615384615384, 'F1': 0.5726141078838175, 'ERDE_5': 0.28534646598882946, 'ERDE_50': 0.12226637381308297, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5458353154415272}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [5, 5, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [5, 5, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.42, 'recall': 0.8076923076923077, 'F1': 0.5526315789473685, 'ERDE_5': 0.31289588271012986, 'ERDE_50': 0.11470471527812945, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5310898679086231}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.48484848484848486, 'recall': 0.7692307692307693, 'F1': 0.5947955390334573, 'ERDE_5': 0.2951402343432478, 'ERDE_50': 0.10614265993774298, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5692944526412838}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5036496350364964, 'recall': 0.6634615384615384, 'F1': 0.5726141078838175, 'ERDE_5': 0.28534646598882946, 'ERDE_50': 0.12226637381308297, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5458353154415272}\n",
      "Writing results to CSV file\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] with weights [1, 5, 1]\n",
      "EVALUATING ENSEMBLE ['svm', 'bayes', 'cnn_model'] WITH WEIGHTS [1, 5, 1] FOR WINDOW SIZES 1, 2 AND 3\n",
      "{'precision': 0.3865546218487395, 'recall': 0.8846153846153846, 'F1': 0.5380116959064328, 'ERDE_5': 0.3302709521142193, 'ERDE_50': 0.11322926971033481, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.517039871403104}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.40384615384615385, 'recall': 0.8076923076923077, 'F1': 0.5384615384615384, 'ERDE_5': 0.3177849487889666, 'ERDE_50': 0.11935460434027145, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5153756991940083}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.4301675977653631, 'recall': 0.7403846153846154, 'F1': 0.5441696113074205, 'ERDE_5': 0.3050995973098398, 'ERDE_50': 0.12311587277635756, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5187210502713381}\n",
      "Writing results to CSV file\n",
      "************ FINISHED EXPERIMENT {'eval_window_size': 3, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 5, 'shuffle': True, 'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile', 'weights': [1, 5, 1], 'model': ['svm', 'bayes', 'cnn_model'], 'eval_time': 50.500592444000176} ************* \n",
      "\n",
      "Initialized numpy random and tensorflow random seed at 79\n",
      "********** CALCULATING FEATURES FOR {'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 75, 'dis_strategy': 'quantile'} ***********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculating features for {'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 75, 'dis_strategy': 'quantile'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA FOR PARAMS {'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 75, 'dis_strategy': 'quantile'}\n",
      "Windowfying training users\n",
      "[====================] 100%\n",
      "Windowfying test users\n",
      "[====================] 100%\n",
      "Oversampling train users\n",
      "After oversample: positive messages: 2792, negative messages: 3491\n",
      "Data size: 6283\n",
      "\n",
      "Finished windowfying\n",
      "Featurizing calculate_feats=True, normalize=True, discretize=True, discretize_size=75, include_feats=['first_prons', 'nssi']\n",
      "Data size: 6283, 6283\n",
      "Data size: 4650, 4650\n",
      "Calculating first prons\n",
      "Calculating NSSI words\n",
      "Calculating first prons\n",
      "Calculating NSSI words\n",
      "Normalizing features\n",
      "Discretizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:188: UserWarning: Feature 4 is constant and will be replaced with 0.\n",
      "  \"replaced with 0.\" % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the combined the same from tfidf: False\n",
      "************ STARTING EXPERIMENT {'eval_window_size': 1, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 5, 'shuffle': True, 'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 75, 'dis_strategy': 'quantile'} ***************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Experiment {'eval_window_size': 1, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 5, 'shuffle': True, 'random_seed': 79, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 20, 'sample_weights_size': 20, 'oversample': True, 'include_new_data': True, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 75, 'dis_strategy': 'quantile'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING AND EVALUATING TRADITIONAL MODEL svm\n",
      "Starting training traditional\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      3602\n",
      "           1       0.51      0.60      0.55      1048\n",
      "\n",
      "    accuracy                           0.78      4650\n",
      "   macro avg       0.69      0.72      0.70      4650\n",
      "weighted avg       0.79      0.78      0.78      4650\n",
      "\n",
      "[[2986  616]\n",
      " [ 418  630]]\n",
      "Evaluating after getting time 4256.656721278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      3602\n",
      "           1       0.51      0.60      0.55      1048\n",
      "\n",
      "    accuracy                           0.78      4650\n",
      "   macro avg       0.69      0.72      0.70      4650\n",
      "weighted avg       0.79      0.78      0.78      4650\n",
      "\n",
      "[[2986  616]\n",
      " [ 418  630]]\n",
      "Evaluated with elapsed time 133.97185154199997\n",
      "EVALUATING FOR WINDOW SIZES 1, 2 AND 3 MODEL svm\n",
      "{'precision': 0.3991031390134529, 'recall': 0.8557692307692307, 'F1': 0.5443425076452599, 'ERDE_5': 0.3233482656442495, 'ERDE_50': 0.11334663469868339, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5231239065871452}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.44751381215469616, 'recall': 0.7788461538461539, 'F1': 0.5684210526315789, 'ERDE_5': 0.30385576036182926, 'ERDE_50': 0.11249713573540661, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.5440507380965321}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.5, 'recall': 0.6346153846153846, 'F1': 0.559322033898305, 'ERDE_5': 0.28418362391906626, 'ERDE_50': 0.12819610012910543, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.533164856755891}\n",
      "Writing results to CSV file\n",
      "TRAINING AND EVALUATING TRADITIONAL MODEL bayes\n",
      "Starting training traditional\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81      3602\n",
      "           1       0.44      0.69      0.54      1048\n",
      "\n",
      "    accuracy                           0.74      4650\n",
      "   macro avg       0.67      0.72      0.68      4650\n",
      "weighted avg       0.79      0.74      0.75      4650\n",
      "\n",
      "[[2696  906]\n",
      " [ 324  724]]\n",
      "Evaluating after getting time 4398.620086374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81      3602\n",
      "           1       0.44      0.69      0.54      1048\n",
      "\n",
      "    accuracy                           0.74      4650\n",
      "   macro avg       0.67      0.72      0.68      4650\n",
      "weighted avg       0.79      0.74      0.75      4650\n",
      "\n",
      "[[2696  906]\n",
      " [ 324  724]]\n",
      "Evaluated with elapsed time 2.8501980700002605\n",
      "EVALUATING FOR WINDOW SIZES 1, 2 AND 3 MODEL bayes\n",
      "{'precision': 0.36220472440944884, 'recall': 0.8846153846153846, 'F1': 0.5139664804469274, 'ERDE_5': 0.3395900806455995, 'ERDE_50': 0.12252904783461957, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.4939319441895574}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.4215686274509804, 'recall': 0.8269230769230769, 'F1': 0.5584415584415584, 'ERDE_5': 0.3142953624690851, 'ERDE_50': 0.11113905515595777, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.534499101761504}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.43103448275862066, 'recall': 0.7211538461538461, 'F1': 0.539568345323741, 'ERDE_5': 0.3033552539031326, 'ERDE_50': 0.12610029676576012, 'median_latency_tps': 13.0, 'median_penalty_tps': 0.046765862163709926, 'speed': 0.9532341378362901, 'latency_weighted_f1': 0.5143349664584299}\n",
      "Writing results to CSV file\n",
      "TRAINING AND EVALUATING DL MODEL cnn_model\n",
      "STARTING ITERATION FOR DL MODEL cnn_model FOR 5 ITERATIONS\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 2s 9ms/step - loss: 0.5284 - tp: 487.0000 - fp: 414.0000 - tn: 3188.0000 - fn: 561.0000 - accuracy: 0.7903 - precision: 0.5405 - recall: 0.4647 - f1_metric: 0.3004\n",
      "Test Score: 0.5284298062324524\n",
      "Test Accuracy: 487.0\n",
      "146/146 [==============================] - 1s 7ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      3602\n",
      "           1       0.54      0.46      0.50      1048\n",
      "\n",
      "    accuracy                           0.79      4650\n",
      "   macro avg       0.70      0.67      0.68      4650\n",
      "weighted avg       0.78      0.79      0.78      4650\n",
      "\n",
      "[[3188  414]\n",
      " [ 561  487]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.45977011494252873, 'recall': 0.7692307692307693, 'F1': 0.5755395683453237, 'ERDE_5': 0.3001622273062801, 'ERDE_50': 0.11137378513265243, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5531048984043213}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 2s 9ms/step - loss: 0.5877 - tp: 530.0000 - fp: 582.0000 - tn: 3020.0000 - fn: 518.0000 - accuracy: 0.7634 - precision: 0.4766 - recall: 0.5057 - f1_metric: 0.3169\n",
      "Test Score: 0.5876911878585815\n",
      "Test Accuracy: 530.0\n",
      "146/146 [==============================] - 1s 7ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      3602\n",
      "           1       0.48      0.51      0.49      1048\n",
      "\n",
      "    accuracy                           0.76      4650\n",
      "   macro avg       0.67      0.67      0.67      4650\n",
      "weighted avg       0.77      0.76      0.77      4650\n",
      "\n",
      "[[3020  582]\n",
      " [ 518  530]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.3789954337899543, 'recall': 0.7980769230769231, 'F1': 0.5139318885448916, 'ERDE_5': 0.32458263312199315, 'ERDE_50': 0.128693504127337, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.4938987006881312}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Evaluating\n",
      "146/146 [==============================] - 2s 9ms/step - loss: 0.4838 - tp: 442.0000 - fp: 294.0000 - tn: 3308.0000 - fn: 606.0000 - accuracy: 0.8065 - precision: 0.6005 - recall: 0.4218 - f1_metric: 0.2854\n",
      "Test Score: 0.4838119149208069\n",
      "Test Accuracy: 442.0\n",
      "146/146 [==============================] - 1s 7ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88      3602\n",
      "           1       0.60      0.42      0.50      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.72      0.67      0.69      4650\n",
      "weighted avg       0.79      0.81      0.79      4650\n",
      "\n",
      "[[3308  294]\n",
      " [ 606  442]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.5227272727272727, 'recall': 0.6634615384615384, 'F1': 0.5847457627118643, 'ERDE_5': 0.28219174793155205, 'ERDE_50': 0.11936019314923905, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5619522331139667}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Evaluating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 3s 9ms/step - loss: 0.4869 - tp: 431.0000 - fp: 264.0000 - tn: 3338.0000 - fn: 617.0000 - accuracy: 0.8105 - precision: 0.6201 - recall: 0.4113 - f1_metric: 0.2769\n",
      "Test Score: 0.4869339168071747\n",
      "Test Accuracy: 431.0\n",
      "146/146 [==============================] - 1s 8ms/step\n",
      "Entered here\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      3602\n",
      "           1       0.62      0.41      0.49      1048\n",
      "\n",
      "    accuracy                           0.81      4650\n",
      "   macro avg       0.73      0.67      0.69      4650\n",
      "weighted avg       0.79      0.81      0.80      4650\n",
      "\n",
      "[[3338  264]\n",
      " [ 617  431]]\n",
      "Finished training and evaluation\n",
      "{'precision': 0.53125, 'recall': 0.6538461538461539, 'F1': 0.5862068965517242, 'ERDE_5': 0.28043428758916694, 'ERDE_50': 0.11998055094479108, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.5633564116075049}\n",
      "Starting training deep model cnn_model\n",
      "Starting training with model_name=cnn_model and maxlen=1000 and batch size=32\n",
      "Generating embeddings\n",
      "Data size: 6283\n",
      "Training with callback\n"
     ]
    }
   ],
   "source": [
    "firstpart_generator = traverse(first_part)\n",
    "\n",
    "for i in firstpart_generator:\n",
    "    try:\n",
    "        # experiment object defined in experiment_utils.py\n",
    "        experiment = Experiment(models, ensemble_combinations, eval_filename, random_seed=i[\"random_seed\"])\n",
    "        \n",
    "        logger(\"********** CALCULATING FEATURES FOR {} ***********\".format(i))\n",
    "        display(Markdown(\"#### Calculating features for {}\".format(i)))\n",
    "                \n",
    "        experiment.prepare_data(i)\n",
    "\n",
    "        secondpart_generator = traverse(second_part)\n",
    "\n",
    "        for j in secondpart_generator:\n",
    "            params = j.copy()\n",
    "            params.update(i)\n",
    "            logger(\"************ STARTING EXPERIMENT {} ***************\".format(params))\n",
    "            display(Markdown(\"#### Experiment {}\".format(params)))\n",
    "            try:\n",
    "                experiment.train_and_evaluate_model(params, weights)\n",
    "                logger(\"************ FINISHED EXPERIMENT {} ************* \\n\".format(params))\n",
    "            except Exception as e:\n",
    "                logger(\"*************************************\")\n",
    "                logger(\"Error during experiment {}: {}\".format(params, e))\n",
    "                logger(\"*************************************\")\n",
    "        del secondpart_generator\n",
    "    except Exception as e:\n",
    "        logger(\"*************************************\")\n",
    "        logger(\"General error during experiment {}: {}\".format(i, e))\n",
    "        logger(\"*************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
