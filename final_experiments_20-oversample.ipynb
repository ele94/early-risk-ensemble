{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from experiment_utils import *\n",
    "from preprocessing import preprocess\n",
    "from windowfy import windowfy\n",
    "from featurizing import featurize\n",
    "from tfidf_featurizer import combine_features, tfidf_featurize\n",
    "from training import train, do_ensemble, do_train\n",
    "from training_traditional import train_and_evaluate\n",
    "from eval_erisk import evaluate, ensemble_vote\n",
    "from IPython.display import display, Markdown\n",
    "from itertools import product\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized numpy random and tensorflow random seed at 42\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "tensorflow.random.set_seed(42) \n",
    "logger(\"Initialized numpy random and tensorflow random seed at 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "first_part = {\n",
    "    \"include_feats\": [[\"first_prons\", \"nssi\"],[\"first_prons\",\"sentiment\",\"nssi\"]],\n",
    "    \"feat_window_size\": [10], #10\n",
    "    \"max_size\": [20],\n",
    "    \"sample_weights_size\": [20],\n",
    "    \"oversample\": [True],\n",
    "    \"include_new_data\": [False],\n",
    "    \"tfidf_max_features\": [5000, 50000],\n",
    "    \"scale\": [False, True],\n",
    "    \"normalize\": [True, False],\n",
    "    \"discretize\": [True, False],\n",
    "    \"discretize_size\": [50, 75],\n",
    "    \"dis_strategy\": [\"quantile\"]\n",
    "}\n",
    "\n",
    "second_part = {\n",
    "    \"eval_window_size\": [1],\n",
    "    \"maxlen\": [1000],\n",
    "    \"batch_size\": [32],\n",
    "    \"epochs\": [100],\n",
    "    \"patience\": [10],\n",
    "    \"iterations\": [5],\n",
    "    \"shuffle\": [True, False],\n",
    "}\n",
    "\n",
    "models = [\"svm\", \"bayes\", \"cnn_model\"]\n",
    "ensemble_combinations = [[\"svm\", \"bayes\", \"cnn_model\"]]\n",
    "weights = [[1, 1, 1], [2, 1, 1], [1, 2, 1], [2, 2, 1], [1, 1, 2], [3, 3, 1], [5, 5, 1], [1, 5, 1]]\n",
    "eval_filename = \"experiments_20_oversample.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** CALCULATING FEATURES FOR {'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 10, 'sample_weights_size': 10, 'oversample': True, 'include_new_data': False, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'} ***********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Calculating features for {'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 10, 'sample_weights_size': 10, 'oversample': True, 'include_new_data': False, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowfying training users\n",
      "[====================] 100%\n",
      "Windowfying test users\n",
      "[====================] 100%\n",
      "Oversampling train users\n",
      "After oversample: positive messages: 478, negative messages: 598\n",
      "Data size: 1076\n",
      "\n",
      "Finished windowfying\n",
      "Featurizing calculate_feats=True, normalize=True, discretize=True, discretize_size=50, include_feats=['first_prons', 'nssi']\n",
      "Initialized numpy random and tensorflow random seed at 42\n",
      "Data size: 1076, 1076\n",
      "Data size: 841, 841\n",
      "Calculating first prons\n",
      "Calculating NSSI words\n",
      "Calculating first prons\n",
      "Calculating NSSI words\n",
      "Normalizing features\n",
      "Discretizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:188: UserWarning: Feature 4 is constant and will be replaced with 0.\n",
      "  \"replaced with 0.\" % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/home/ecampillo/.local/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:222: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the combined the same from tfidf: False\n",
      "************ STARTING EXPERIMENT {'eval_window_size': 1, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 10, 'shuffle': True, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 10, 'sample_weights_size': 10, 'oversample': True, 'include_new_data': False, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'} ***************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Experiment {'eval_window_size': 1, 'maxlen': 1000, 'batch_size': 32, 'epochs': 100, 'patience': 10, 'iterations': 10, 'shuffle': True, 'include_feats': ['first_prons', 'nssi'], 'feat_window_size': 10, 'max_size': 10, 'sample_weights_size': 10, 'oversample': True, 'include_new_data': False, 'tfidf_max_features': 5000, 'scale': False, 'normalize': True, 'discretize': True, 'discretize_size': 50, 'dis_strategy': 'quantile'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training traditional\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       636\n",
      "           1       0.79      0.15      0.25       205\n",
      "\n",
      "    accuracy                           0.78       841\n",
      "   macro avg       0.79      0.57      0.56       841\n",
      "weighted avg       0.78      0.78      0.72       841\n",
      "\n",
      "[[628   8]\n",
      " [175  30]]\n",
      "{'precision': 0.7692307692307693, 'recall': 0.19230769230769232, 'F1': 0.3076923076923077, 'ERDE_5': 0.2492555420179793, 'ERDE_50': 0.20206897708029445, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.2956983879930795}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.8333333333333334, 'recall': 0.09615384615384616, 'F1': 0.1724137931034483, 'ERDE_5': 0.2470038185729965, 'ERDE_50': 0.22338469448775772, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.16502177560399284}\n",
      "Writing results to CSV file\n",
      "{'precision': 0, 'recall': 0, 'F1': 0, 'ERDE_5': 0.2458628841607565, 'ERDE_50': 0.2458628841607565, 'median_latency_tps': nan, 'median_penalty_tps': nan, 'speed': nan, 'latency_weighted_f1': nan}\n",
      "Writing results to CSV file\n",
      "Starting training traditional\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       636\n",
      "           1       0.57      0.38      0.45       205\n",
      "\n",
      "    accuracy                           0.78       841\n",
      "   macro avg       0.69      0.64      0.66       841\n",
      "weighted avg       0.76      0.78      0.76       841\n",
      "\n",
      "[[578  58]\n",
      " [128  77]]\n",
      "{'precision': 0.51, 'recall': 0.49038461538461536, 'F1': 0.5, 'ERDE_5': 0.2740970202136169, 'ERDE_50': 0.1537760787798512, 'median_latency_tps': 11.0, 'median_penalty_tps': 0.03898023902249159, 'speed': 0.9610197609775084, 'latency_weighted_f1': 0.4805098804887542}\n",
      "Writing results to CSV file\n",
      "{'precision': 0.7428571428571429, 'recall': 0.25, 'F1': 0.37410071942446044, 'ERDE_5': 0.2510380109370978, 'ERDE_50': 0.1896282883154771, 'median_latency_tps': 12.0, 'median_penalty_tps': 0.042873701496841665, 'speed': 0.9571262985031583, 'latency_weighted_f1': 0.3580616368501024}\n",
      "Writing results to CSV file\n",
      "{'precision': 0, 'recall': 0, 'F1': 0, 'ERDE_5': 0.2458628841607565, 'ERDE_50': 0.2458628841607565, 'median_latency_tps': nan, 'median_penalty_tps': nan, 'speed': nan, 'latency_weighted_f1': nan}\n",
      "Writing results to CSV file\n",
      "Starting training deep model\n",
      "Starting training with cnn_model=cnn_model and maxlen=1000 and batch size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib64/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(models, ensemble_combinations, eval_filename)\n",
    "\n",
    "firstpart_generator = traverse(first_part)\n",
    "\n",
    "for i in firstpart_generator:\n",
    "    try:\n",
    "        logger(\"********** CALCULATING FEATURES FOR {} ***********\".format(i))\n",
    "        display(Markdown(\"#### Calculating features for {}\".format(i)))\n",
    "        \n",
    "        experiment.prepare_data(i)\n",
    "\n",
    "        secondpart_generator = traverse(second_part)\n",
    "\n",
    "        for j in secondpart_generator:\n",
    "            params = j.copy()\n",
    "            params.update(i)\n",
    "            logger(\"************ STARTING EXPERIMENT {} ***************\".format(params))\n",
    "            display(Markdown(\"#### Experiment {}\".format(params)))\n",
    "            try:\n",
    "                experiment.train_and_evaluate_model(params, weights)\n",
    "                logger(\"************ FINISHED EXPERIMENT {} ************* \\n\".format(params))\n",
    "            except Exception as e:\n",
    "                logger(\"*************************************\")\n",
    "                logger(\"Error during experiment {}: {}\".format(params, e))\n",
    "                logger(\"*************************************\")\n",
    "        del secondpart_generator\n",
    "    except Exception as e:\n",
    "        logger(\"*************************************\")\n",
    "        logger(\"General error during experiment {}: {}\".format(i, e))\n",
    "        logger(\"*************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
